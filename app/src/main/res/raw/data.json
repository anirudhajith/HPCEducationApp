{
  "subjects": [
    {
      "name": "High Performance Computing",
      "content": [
        {
          "title": "System Calls",
          "videoId": "7cfOMkcwX64",
          "transcript": "So how does a process gets the operating system to do something for it? A process or programs in execution when they require functionality from the operating system will do so by calling function like entities known as system calls which are interfaces into the operating system for example there is one of the system call known as fork() .The effect is due to it a new process comes into existence and now there are two processes .First the original process we referred to as a parent process and second the new process we will refer to as a child process. In fact now from this point both the parent and the child process execute the same program. They are two processes which are now associated with the execution of a single program.\n\nNow who doesn’t want priveleges??\nA process must be allowed to do sensitive operations while it is executing a system call . Special instructions have been included in the Instruction Set Architecture for such purposes. They are called privileged instructions.\n\nMechanics of System Calls--\nProcessor hardware is designed to operate in at least 2 modes of execution  \n1)Ordinary or user mode .The processor cannot run privileged instructions ie can only run ordinary MIPS 1 instructions.\n2)Privileged or system mode .The processor is capable of running privileged instructions. So if a program is executing inside the system call/or in operating system it will be in the system mode. Now how does my hardware knows what kind of mode the processor is in?? The processor status register tells the hardware what state the program is running in so if there is any violation of status, an error is thrown. So atlast now if I write a program which makes a system call and as system calls itself contains privileged instructions then in order to execute in the system call my program has to change mode from user mode to system mode and there's no other way that my program can execute the system call because the system call code is basically going to contain privileged instructions. So now how can this change of mode take place- We have to do it explicitily by executing a system call which is entered using a special machine instruction (e.g. MIPS 1 syscall) that switches processor mode from user to system before transferring control. In above example the syscall/system call function must be an ordinary function and not privileged one as it must be available to user programs. \n\nNow let us take another example- If we want to write a program to copy the content of one file into another then, first of all this program will need the names of these files. User will give these names by either typing them in the console or selecting them by using GUI. So our program will need to make system calls to the kernel to enable it to access the input and output devices. Also our program will need to display certain message if the program is successfully completed or even if it stops and is aborted. All these tasks require System calls.\n\nSome other examples of system calls\nExamples: Operations on files\ncreate(): to create a new file\nunlink(): to remove a file\nopen(): to open a file for reading and/or writing\nread(): to read data from an open file into a variable\nwrite(): to write data into an open file\nlseek(): to change the current pointer into the open file\nExamples of system calls: Operations on processes \nfork(): to create a new process \nexec(): to change the memory image of a process \nexit(): to terminate \nwait(): to make parent sleep until child terminate",
          "valid": true
        },
        {
          "title": "Virtual Memory",
          "videoId": "_1tnoez75ws",
          "transcript": "Imagine you are working on mechanics problems. Your desk is full of physics books, your daily planner, calendar, journal and notebooks etc. Now let’s say your teacher calls you and tells you that you have a test on OS architecture tomorrow. What would you do? You would take the physics books and keep them in your cupboard and fill the desk with books on OS. Similarly, whenever your computer doesn't have space in the physical memory it writes what it needs to remember in file as virtual memory that it can access whenever there is demand. By definition Virtual memory is a storage allocation scheme by which a computer can access the secondary memory (hard drive, optical disks) as though it was a part of the main memory (RAM). In Virtual Memory, memory addresses that a program may use are stored as virtual addresses that are translated to physical addresses whenever necessary. Secondly, It allows us to have memory protection, because it prevents a process from accessing memory that has not been allocated to it. \n\nNow lets see how virtual memory works. So when a system usually executes a program, stores or retrieves data it looks up the corresponding memory address of that program. A digital computer's main memory consists of many memory locations. Each memory location has a physical address which is a code.\n\nVirtual memory permits our program to use more pages of memory than that can fit into the RAM at any given time. Pages are roughly speaking blocks of data. Instead of preventing pages from entering the main memory, the OS swaps an unused page in the main memory to make space for the new pages. All memory references within a process are logical addresses that are dynamically translated into physical addresses at run time. it is not necessary that all the pages or segments of the process need to be present in the main memory during execution. This means that the required pages need to be loaded into memory from virtual memory whenever required. Virtual memory is implemented using Demand Paging or Demand Segmentation.\n\nSo without virtual memory, program address is exactly the same as ram address. So here is our 32 Bit address space of the program of 4 gb. And we have 30 Bit RAM in our computer of 1 gb. So now the OS can map these addresses just fine. It can map 0 address to 0, 1 to 1, 2 to 2 etc. But what happens when we try to access addresses that are beyond 1 gb They don’t have anywhere to map to. So the program is gonna crash. What virtual memory does is that until the first gigabyte is filled it can map to anywhere it wants within this 1 gb space program address 0 can map to RAM address 1. So say the next one maps up to RAM address 0. Then the next one it maps up to RAM address 2. What happens now when we access this next piece of data. Well now the map says its not in virtual memory. Its somewhere else. So a PAGE FAULT OCCURS\n\nPage fault is when the running program tries to access a memory location that is not yet mapped in virtual memory So the OS searches for the required page in memory space, and say it finds it in the disk for example. So the program won't crash but what it will do is Swap the oldest page in the RAM that is the RAM address 1 in this case and put it in the disk And take the page in demand, that is the program address 3 and Moves it to disk. AND maps the program address 3 to RAM address 1. So this is called swapping The pages of memory which are not currently being used are stored in the hard disk (slower memory) and brought into the RAM (faster memory) whenever the program needs it. The process of loading the page into memory on demand (whenever page fault occurs) is known as demand paging.\n\nBecause of virtual memory we can run programs on smaller computers (less RAM) which typically would require a lot of RAM.\n\nAlthough the virtual memory will be much slower than RAM, this is a better alternative to either optimizing the program to use less RAM or even buying new RAM which can be very expensive. Another advantage of virtual memory is multitasking, which enables simultaneous loading of memory of more programs that can fit into the RAM. Example- Opening multiple windows in your OS. Like internet explorer and some application software like paint. Even though both these softwares are using significant amount of memory, whichever application is receiving input will reside in the main memory, the rest of the applications can be paged out to the secondary memory.",
          "valid": true
        },
        {
          "title": "Registers",
          "videoId": "u1zrm8P0sgM",
          "transcript": "The most fundamental unit of computer memory is the bit. Each of these bits is stored in a memory cell that can switch between two states 0 and 1 like a switch goes ‘on’ and ‘off’.\n\nComputers are made of devices that can flip between the states of 0 and 1. \nTraditionally we have gone from using relays,vacuum tubes to the transistors which are the most prevalent devices we use today: the essence of all these devices is that they can switch between two states. \n\nNow that we have devices that switch between two states we can construct logic gates. lets construct an ordinary OR gate and feed the output back into one of its inputs. First if both inputs are set to 0, 0 or 0 will give the output 0, say we switch input A to 1 now the output becomes 1. This will loop back into input B, such that both inputs are now 1, 1 or 1 is still 1. There is no change in the output. If you again change the input A to 0 the circuit still outputs 1. Now we have a circuit that records 1. Except that we have a problem : This change is permanent. Since the output is now 1, no matter the input there isn’t a way to change the output to zero. \n\nNow we will take an AND gate with its output looping back into its input again. If   we start with both inputs at 1, the output will be 1. 1 and 1 outputs one forever. Unless you change A to 0. Since it's an AND gate now the output will change to 0. The output will loop back to 0. Again we face the same issue as the  previous circuit. The output  now being 0, cannot change back to 1 no matter what the input. \n\nThis is called a latch because it latches onto a particular value and stays that way.\n\nSince having 2 wires for input  is a bit confusing we can modify this circuit a bit such that we have a GATED LATCH. A gated latch will have a wire called DATA INPUT that we can set to 0 or 1 to input a bit and another wire WRITE ENABLE to open the latch for modification and to close the latch to store the bit. \n\nA register consists of a group of latches  with each latch capable of storing one bit of information. An n-bit register has a group of n latches and is capable of storing binary information of n-bits. The latches  hold the binary information and gates control when and how new information is transferred into a register. \n\nIn short, A processor register is a quickly accessible location available to a computer's processors. \nA register can hold an instruction, a storage address, and any type of data like a bit sequence or individual character.\n\nProcessor register is one among many data holding places such as L1 CACHE, L2 CACHE, L3 CACHE, RAM, SSD. These registers are part of the computer processor. Register memory is the smallest and fastest memory in a computer. It is not a part of the main memory and is located in the CPU.\nA register temporarily holds frequently used data, instructions, and memory address that are to be used by the CPU. They hold instructions that are currently processed by the CPU. All data is required to pass through registers before it can be processed.\n\nRegisters are normally measured by the number of bits they can hold, for example, an \"8-bit register\", \"32-bit register\" or a \"64-bit register\" or even more\nA register must be large enough to hold an instruction - for example, in a 64-bit computer, a register must be 64 bits in length. In some computer designs, there are smaller registers - for example, half-registers - for shorter instructions. Registers hold a small amount of data around 32 bits to 64 bits. The speed of a CPU depends on the number and size (no. of bits) of registers that are built into the CPU. \nAlmost all computers load data from a larger memory into registers where it is used for arithmetic operations and is manipulated or tested by machine instructions. Manipulated data is then often stored back into the main memory. \n\nWhen a computer program accesses the same data repeatedly, this is called locality of reference. Holding frequently used values in registers can be critical to a program's performance.\nRegisters are the essential component in computers because CPU cannot directly access data that is stored in the memory and it must be first pass through the registers. Whenever CPU gets the data or executes the instruction code, a cycle known as Fetch-Decode-Execute Instruction Cycle takes place\n\nThe computer needs processor registers (OTHERWISE KNOWN AS ACCUMULATOR) for manipulating data and an Address register for holding a memory address. The register holding the memory location of  the address of the next instruction that is to be executed when the current instruction is completed is called PROGRAM COUNTER. aN INSTRUCTION REGISTER is the register that holds the instruction currently being executed  encoded or decoded. \nIt is useful to note that Some instructions specify registers as part of the instruction. For example, an instruction may specify that the contents of two defined registers be added together and then placed in a specified register.",
          "valid": true
        },
        {
          "title": "Addressing Modes",
          "videoId": "0hUFoNOXMsk",
          "transcript": "The job of a processor is to execute a set of instructions using data stored in memory to perform a particular task. What are instructions?\n\nInstructions are a segment of code containing steps that need to be executed by the processor. It can be divided into three parts:\n\nAddressing mode, we will define it later, for now, keep in mind its size is 1 bit.\nThe opcode is the portion of an instruction that specifies the operation to be performed.\nThe operand is the object on which the operations are performed\n\nSay you give your system an instruction “Add 2 and 5”.\n\n\nThe values 2 and 5 are called operands.\nBy definition: The object and quantity that is operated on is called an “operand”.\n\nHere add is the operator\nThe operator says what to do with operands\n\n\nSo how will the cpu get the operands to perform the operation?\n\nThis is where addressing modes come in\nThe processor needs to be told how to get the operands for an instruction.\nWe use addressing modes to tell the processor how to get operands for an instruction.\n\nAddressing modes is the method to specify the operand of an instruction.\n\n“Addressing modes in a given instruction set architecture define how the machine language instructions in that architecture identify the operand(s) of each instruction.”\n\nIt defines how the CPU gets values from registers. And what needs to be done by the CPU incase the operands need to be specified from main memory.\n\n\nAn addressing mode tells the processor how to get the memory address of an operand\nWhether the address is held in registers\nOr is contained within a machine instruction in the form of constants\nor elsewhere.\nNow there are several types of addressing modes\n\n\nImmediate addressing mode\n\nIn immediate addressing mode the data to be used as the operand is included in the instruction itself. That is, the operand is specified in the instruction itself.\n\nIn this mode, the data is present in the address field. i.e  The value of the operand  is immediately accessed from the address field of the instruction.\nBut there is a caveat\n\nIn immediate mode the range of constants (that is the value of the operands)  are restricted by size of address field.\n\nAn Example for this mode would be an instruction to move the data 49B into AX register. Here the operand 49B is specified in the instruction itself\n\nirect Addressing Mode\nIn this type of addressing mode the memory address of the operand is directly specified in the instruction as a part of it.\n\n\n\nHere the operand is stored in the memory address 5000. The cpu goes to the memory address 5000 and fetches the operand stored that is “64”\nRegister Direct Mode\n\nIn this mode, the data of the operand is stored in the register i.e, within the CPU\nFor Example, ADD R1, R2\n\nThe address field of the instruction refers to a CPU register that contains the operand.\nTo get the  operand there is no memory access involved\nRegister Indirect Mode\n\nIn this mode, the instruction specifies the register which has the address of operand.\nThe operand itself is stored in memory and the instruction only specifies to the register which inturn points to the memory location\nThus, the register contains the address of operand rather than the operand itself.\n\n\n\nIndexed Addressing Mode\nThe address of the operand is obtained by adding to the contents of the general register (called index register) a constant value. The number of the index register and the constant value are included in the instruction code. Index Mode is used to access an array whose elements are in successive memory locations.\nThe content of the instruction code, represents the starting address of the array and the value of the index register, and the index value of the current element.\n\nIn this addressing mode, offset of the operand is stored in one of the index registers. DS is the default segment for index register SI and DI.\n\nExample - MOV AX, [SI]\n\n\nRegister Relative addressing mode\n\nIn this mode, the data is available at an effective address formed by adding an 8-bit or 16-bit displacement with the content of any one of the registers BX, BP, SI and DI in the default (either DS or ES) segment.\n\nExample : MOV AX,50H[BX]\n\n\n\nBase plus index addressing mode\nIn this mode the effective address is formed by adding content of a base register (any one of BX or BP) to the content of an index register (SI or DI).\nDefault segment register is DS.\n\nExample MOV AX, [BX][SI]\n\n\n Base relative plus index addressing mode\n\nIn the effective address is formed by adding an 8 or 16-bit displacement with sum of contents of any one of the base registers (BX or BP) and any one of the index registers, in a default segment.\n\nExample - MOV AX,50H[BX][SI]",
          "valid": true
        },
        {
          "title": "Types Of Instructions",
          "videoId": "INBx_PzOjHY",
          "transcript": "To recall from our last video, Instructions are a segment of code containing steps that need to be executed by the processor. It can be divided into three parts:Addressing mode The opcode - the portion of an instruction that specifies the operation to be performed. The operand -  the object on which the operations are performed Say you give your system an instruction “Add 2 and 5”. The values 2 and 5 are called operands. By definition: The object and quantity that  is operated on is called an “operand”.Here add is the opcodeThe opcode says what to do with operandsIn this video we will look into some of the types of instructionsInstructions can be broadly divided into three categories:Instruction types[edit]• firstly there are Data transfer instructions: instructions that move data from and into registers, main memory, stack or I/O• then there are Data processing instructions that perform arithmetic and logical operations •lastly there are Control instructions that help in systems control and transfer of control and help keep the functioning of cpu smooth1. Data Transfer InstructionsAs from the name, these instructions are used to transfer data from a source operand to a destination operand. The operands can be anything, from a register to a memory location.  As said before these instructions move, store, load data from and into registers, main memory, stack or I/OThese instructions can be used toCopy data from a memory location to a register, or vice versa (the instruction MOVE is used for this.AN EG OF DATA TRANSFER INSTRUCTION WOULD BE  MOV BX, AX THIS INSTRUCTION Copies value in AX register to BX registerANOTHER EXAMPLE LDS BX, 4326 loads content at memory Address 4326 to register BXMOV AX, 5000A moves value in address 5000A to AX register instructs the system to -----------------The instruction store is Used to store the contents of a register or the result of a computation, and the instruction load is used to retrieve stored data to perform a computation on it later.For instance Load _____ says the system to ______Store says the system to _______Store _____ says the system to _______2. Data processing / Data manipulation instructionsThere are three further types of data manipulation instructionsThe first one isArithmetic instructionsThese instructions are used to perform arithmetic operations like addition, subtraction, multiplication, division, etc.An example of arithmetic instruction would beADD AL, 74Hthis instruction adds number In address 74 H To the content of ALANOTHER EXAMPLE OF ARITHMETIC EXAMPLE WOULD BEDIV BL THIS INSTRUCTION DIVIDES THE WORD IN THE REGISTER AL BY THE BYTE IN BL It may also include – Increment (i.e a++) – Decrement (i.e a--) – Negate (i.e -a)ADDITION WITH CARRY AND SUBTRACTION WITH BORROWSecondly we have Logic and bit manipulation instructions such asand, or, xor, notAn example of logical instruction would be And BH, CL This instruction. Would Compute logical and on the registers BH AND CL and store the result in BH register NOT BX  This instruction  Inverts content of BX registerLastly  there areShift and rotation instructionsThe purpose of each shift instruction is to shift an operand bit by bit to the right or leftIn shl - logical shift left The highest order bit is shifted out. Then each bit is shifted one bit to the left. Then 0 is shifted in. In shr - the logical shift right the lowest order bit is shifted out. Then the remaining bits are shifted to the right one by one. Then 0 is shifted in. In the Next category we have Control instructionsGenerally CPU executes instructions in sequence Program control instructions dictate the sequence in which the program is executed. it provides control over the flow of the program and provides the capability tbranch to different program segments.  Control flow instructionsSOME OF THE common instructions under control flow instructions are  Unconditional Branch Instruction:This instruction  instructs the control flow to Branch to another location in the program and execute instructions there.  Conditional Branch Instruction:This instruction  instructs the control flow to Conditionally branch to another location if a certain condition holds.Call This instruction Calls another block of code, while saving the location of the next instruction as a point to return to.in call Program Control is transferred to a memory location which is not a part of main programJump This instruction lets program to jump to different segments of the program (functions, subroutines, etc.)Unlike call, In jump Program control is transferred to a memory location which is in the main programAn Example for control instruction would be Compare AX, 4371 hJump if Above to nextThis Instruction Subtracts 4371 h from AX and jumps to the label Next if  AX is above 4371hThe next example call BXTransfers the program control to the register BX which is not a part of the main program that is currently being executed",
          "valid": true
        },
        {
          "title": "ISA",
          "videoId": "qaquXB_OQ7U",
          "transcript": "Imagine being a programmer who wants to give an instruction to the computer to perform an operation. Say you want to add 1 + 1. \nCommunicating this instruction entirely in terms of logic would be highly tedious and time consuming. \n//high level: 1+1 = 2 \n//Low level: and and or gate combined to perform addition \nInstead we try to express our programs in a way that is easier for humans to understand.\n\nFormally, The Instruction Set Architecture (ISA) is the part of the processor that is visible to the programmer or compiler writer. ISA defines all of the programmer-visible components and operations of the computer\n \nISA is a well defined hardware and software interface. It provides \nfunctional definition of operations, modes, locations supported by hardware \nPrecise description of how to invoke and access t]hem\nThe ISA of a processor generally  describes\nOperand Storage in the CPU\nWhere are the operands kept other than in memory? \nNumber of explicit named operands\nHow many operands are named in a typical instruction. \nOperand location\nmust all operands be kept internally in the CPU? \nCan any ALU instruction operand be located in memory? Operations\nWhat operations are provided in the ISA. \nType and size of operands\nWhat is the type and size of each operand and how is it specified?\nThe 3 most common types of ISAs grouped according to how they store their operands are: \n \nStack \nAccumulator - One operand is implicitly the accumulator.\nGeneral Purpose Register (GPR) - All operands are explicitly mentioned, they are either registers or memory locations. \n\nC = A + B;\nin all 3 architectures:\n\nA stack is a basic data structure that can be logically thought of as a linear structure represented by a pile of plates. We can only access the plate at the top of the pile. ie. only the data at the top of the stack can be accessed. In stack the operands are implicitly on top of the stack.\nBecause the operands used in the instructions are always in a known location (the top of the stack), the instructions themselves do not require memory addresses or register numbers to specify their operands. This leads to an instruction set architecture (ISA) style known as a zero address format\n\n\n* An accumulator-based  architecture is an architecture that only has one general purpose register (ie. the accumulator). It is an implicit single element storage model. \n* A GPR based CPU architecture is a  register-based  architecture that has one or more general purpose registers \n\nIn an accumulator architecture, most instructions operate on a single register called the accumulator. \nIn a gpr architecture, instructions operate on multiple general purpose registers\n\nSince there is only one register ie. the accumulator in this model, Most operations have the accumulator as an implicit argument to the instruction. This means that you don’t have to specify where the operands go, because there is only a single register - the accumulator.\n \nIn contrast, in a GPR model since there are multiple registers, the registers are explicitly specified in the instructions. \n\n \nAdvantages: operands need not be specified leading to smaller instructions.\nDisadvantages: most of the instructions depend on the result from the previous instruction, and therefore can't be done in parallel. \n The accumulator is only temporary storage so memory traffic is the highest for this approach.\n \nAdvantages: it's easier for the CPU to do more independent instruction in parallel.\nDisadvantages: All operands must be specified leading to longer instructions.  \n \n \nEarlier CPUs were of the first 2 types but in the last few decades all CPUs made are GPR processors. The 2 major reasons are that registers are faster than memory, the more data that can be kept internally in the CPU the faster the program will run. The other reason is that registers are easier for a compiler to use.\n\nMost ISAs allow you to work with a cpu as well as its components using simple instructions. Each instruction will contain an opcode (this opcode tells the cpu what operation are you trying to perform), the rest of the instruction contains operands (what the instruction works on) and information about addressing mode. \n\n\n\nImplementability \n    Certain ISA features such as \nVariable instruction lengths and formats (complicate decoding)\nImplicit state \nVariable latencies \nDifficult to interrupt instructions\nMake the program less efficient to be implemented. \n\nCompatibility \nBackward compatibility (the ability of new processors to support old programs) is very important. There is resistance to buy new hardware if it requires new software. The x86 still remains as one of the most commonly used ISA because of its high compatibility\n\n\nISA can also be classified into RISC and CISC \n\nRISC Reduced Set Instruction Set Architecture (RISC) –\nComplex Instruction Set Architecture (CISC) –\n",
          "valid": true
        },
        {
          "title": "Floating Point Arithmetic",
          "videoId": "g4gUSc90nB0",
          "transcript": "Recall how we represent a normal integer in base 10. The number 185 is a representation of 10^0 *5. + 10^1 * 8 +10^2 *1\n Similarly in base 2.—-a number is represented by the summation of each bit multiplied by 2 to the power its place. such that the bit in the zeros place is multiplied by 2^0 the bit in ones place is multiplied by 2^1, the bit in the 2’s place is multiplied by 2 to the power of 2 and so on. All these products are then summed to give the number in decimal base 10.\nAnd to represent fractional numbers you can follow something similar. when you read a binary fraction it’s read such the each bit on the right side of the decimal is multiplied by 2 to the corresponding negative power abd the bits on the left side will have normal powers of 2. So 1011.1001 is 11.5625 in decimal.\nThis is a simple enough scheme but there is a problem \nOriginally we could represent \nzero to 255 with an unsigned 8 bit number \nbut now because of us assigning a fixed point in the middle our range becomes much smaller. \nYou might say that we can simply add more bits \nBut say a physicist wants to perform a calculation with the plank’s constant. It would be much easier for the physicist to perform calculations with the scientific format rather than this fixed point format. \nSimilarly computers need a large number of bits to work with very large and very small numbers in fixed point format. The floating point system enables representation of both very large numbers and very small numbers without needing enormous amounts of space. \n  Floating point numbers are essentially scientific notation for base 2.\nAccording to the IEEE-754 Floating Point Format\nFloating point numbers are made of 32 or 64 bits depending on their precision. \nLets see how a 32 bit float data type stores numbers. \n\nRemember computers work with the base 2 system \nA floating point number of precision 1 contains 32 bits \nInstead of using the entire 32 bits to represent one number we are gonna break these bits into three parts. \n\nThe 1st bit represents the sign of the number \nThe next eight bits represent the exponent which is 2 ^ of something.\nWhile the remaining 23 bits represent the mantissa which is the actual body of the number \nWe use the sign, the mantissa and the exponent in an equation like this \nto construct a floating point number\nTo get a deeper understanding lets see an example\nlets see how computers construct a floating point number\nIt would be easier to work this backwards so lets start with a floating point number \n1 10001001 00010010000000000000000\nso lets first determine the sign \n0 = positive\n1 = negative\nLets move to exponents \nfirst we will read it as a normal unsigned integer \n10001001 \nwe will get 137 \nRemember that we have a bit to store the sign of the number \nbut we don’t have a bit to store the sign of the exponent \nTo represent both negative and positive exponents the computer offsets the number \n\nNow that we understand the concept of offset\nLets go back and read what our actual exponent is \nThe 8 bits of our floating point number read to 137 \nIn floating point system by convention the exponents are offset by 127\nTherefore the true exponent value is 137-127 = 10 \n\nThe rest of the 23 bits are assigned to store a mantissa \nA mantissa works just like a fixed point number you start to read the first bit from 2^-1 and you have to assume that there is always a 1 in the zeros 0 place followed by a fixed point by default. So though thwre is no bit specifically explicit in the manissa for the zeros place its on by default and hence we should always add one when calculating our mantissa.\n\nNow that we have all our three parts lets put them in the equation and get our decimal . By working all the parts we get our final value which is this decimal value. \n\nThe most vital point to note is that no matter what the mantissa is we can vary to exponents to get a very small or very large number \nTo do the vice versa process that is to convert a fixed point number into a floating point format we first convert the decimal number into binary\nJust like how converting from binary to decimal is repeated multiplication with the corresponding powers of two, decimal to binary conversion is repeated division. so first we go through the already familiar process of conversion of a integer into binary\nProceeding to the decimal part of the number. We again repeat the same process with the decimal part but this time we’d multiply by 2 instead of dividing. Why?  remember each of the bit in the decimal portion is represented by two raised to its corresponding NEGATIVE power \nSince dividing by 2^-1 is the same as multiplying by 2^1 \nwe do repeated multiplication by 2 to get the binary representation of the decimal part.\nIn this process you first multiply the decimal part of the base 10 number by 2. Make note of the integer part of the product. \nNow multiply again by 2, if your product becomes greater than 1, subtract 1 from the product before multiplying with 2 again. \nRepeat this process until you reach 0 in this part.\nBut you may not always get 0 in such cases continue the process until you have values for 23 bits and then stop.  \nNow if we combine our bit string for the integer and the bit representation of the decimal part of our number we get the fixed point binary representation\nSo now we have changed the fixed point base 10 number to a fixed point base 2 number. Now we need to change the fixed point base 2 number to floating point base 2.\nThe first step in this process is to normalize our fixed point bit string \nWhen a number is said to be normalised it means the floating-point is placed to the right of the first non-zero (significant) digit.\nSo lets move our point to the right of the first non zero bit and change our exponent accordingly \nso now we know all the parts of the equation \nThe sign is 0 since our number is positive \nand we know our mantissa, our mantissa is everything that comes after the decimal point (recall that we don’t specify the 1 before the point in our mantissa as it’s assumed to be always there by default) \nTo get our exponent we offset our true exponent by adding +127 \n4 + 127 =  131 which is 1000 0011 in bitstring\nNow we have 3 bit strings for our sign, mantissa and exponent and by combining these we get our floating point representation \n\n\n\n\n\n\n\nNow that we know how floating point numbers work lets move on to floating point arithmetic \n\n\tFloating Point Addition and Subtraction\n\nBasic Steps\nIn addition and subtraction, the radix point is shifted in one of the operands such that both operands have the same exponents. \nThen addition / subtraction takes place \nFinally the result is normalized\n\n\nFlowchart of Addition and Subtraction\n\n\nAddition\n\n\nSubtraction\n\n\n\n\n\n\tFloating Point Multiplication and Division\n\nFirst \n•Check for zero. If none of the operands are zero proceed to\n•Add/subtract exponents \n•Then Multiply/divide significands \n•Normalize \n•and then Round\n•All intermediate results should be in double length storage\n\n\n\n\n\n\nMultiplication Example\n\nMultiply the following two numbers in scientific notation by hand:\n1.230 × 109 × 8.100 × 10-4\nAdd the exponents\nNew Exponent = 9 + (-4) = 5\nIf we add biased exponents, bias will be added twice. Therefore, we need to subtract it once to compensate: \n(9 + 127) + (-4 + 127) = 259 \n259 - 127 = 132 which is (5 + 127) = biased new exponent \nMultiply the mantissas\n 1.230 × 8.100 = 09.963000\n\n Can only keep three digits to the right of the decimal point, so the result is\n\n09.963 × 105\nNormalize the result\n 9.963 × 105\nRound it\n 9.963 × 105\n\n\nDivision Example\n\nDivide the following two numbers in scientific notation by hand:\n1.230 × 109 / 8.100 × 10-4\nSubtract the exponents\nNew Exponent = 9 - (-4) = 13\nIf we subtract biased exponents, bias will be subtracted twice. Therefore, we need to add it once to compensate: \n(9 + 127) - (-4 + 127) = 13\n13 + 127 = 140 which is (13 + 127) = biased new exponent \n Divide the mantissas\n1.230 / 8.100 = 0.1518518\n\n \tCan only keep three digits to the right of the decimal point, so the result is\n\n0.151*1013\n  Normalize the result\n 1.510000 × 1012\nRound it\n 1.510 × 1012\n\n",
          "valid": true
        },
        {
          "title": "ALU",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Cache",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Locality",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Cache Replacement Policy",
          "videoId": "u1xHbfhq0KQ",
          "transcript": "As we all know there are various levels of memory,\n➔ Level 1 or Register :\nRegister memory is the smallest and fastest memory in a computer. Most commonly used registers are accumulator, program counter, address register etc.A register temporarily holds frequently used data, instructions, and memory address that are to be used by CPU.\n➔ Level 2 or Cache memory :\nIt is the fastest memory which has faster access time where data is temporarily stored for faster access.\n➔ Level 3 or Main Memory :\nIt is memory on which computer works currently. It is small in size and once power is off data no longer stays in this memory.\n➔ Level 4 or Secondary Memory:\nIt is external memory which is not as fast as main memory but data stays permanently in this memory.\n\nWhat is cache memory ?\nCache Memory is a special very high-speed memory. It is used to speed up and synchronizing with high-speed CPU.Cache memory is used to reduce the average time to access data from the Main memory.  Cache memory is costlier than main memory or disk memory but economical than CPU registers. Cache memory is an extremely fast memory type that acts as a buffer between RAM and the CPU. It holds frequently requested data and instructions so that they are immediately available to the CPU when needed.\n\nNow let us see about the Cache Performance.\nWhen the processor needs to read or write a location in main memory, it first checks for a corresponding entry in the cache.\n\nCache hit:\nIf the processor finds that the memory location is in the cache, a cache hit has occurred and data is read from cache\n\nCache miss:\nIf the processor does not find the memory location in the cache, a cache miss has occurred. For a cache miss, the cache allocates a new entry and copies in data from main memory, then the request is fulfilled from the contents of the cache.\n\nThe performance of cache memory is frequently measured in terms of a quantity called Hit ratio.\nHit ratio = hit / (hit + miss) = no. of hits/total accesses\n\n\nAs already discussed, during cache miss,the cache allocates a new entry and copies in data from main memory but what is the case if cache is full.In that case,cache replacement algorithms specifies different ways to evict an item from the cache. Why is the optimal cache replacement policy important? The performance of any high performance computing system is highly depending on the performance of its cache memory. It is natural to desire to keep values that would be needed in the near future and discard ones that would not. Therefore the ability to predict future patterns of such access requirements would definitely be a desired algorithm. It ensures the lowest possible miss rate.\n\nNow let us see some of the significant cache replacement policies.\n1. Bélády's algorithm: The most efficient caching algorithm would be to always discard the information that will not be needed for the longest time in the future. This optimal result is referred to as Bélády's optimal algorithm. Since it is generally impossible to predict how far in the future information will be needed, this is generally not implementable in practice.\n2. First in first out (FIFO): Using this algorithm the cache behaves in the same way as a FIFO queue. The cache evicts the blocks in the order they were added, without any regard to how often or how many times they were accessed before.\n3. Last in first out (LIFO) or First in last out (FILO): Using this algorithm the cache behaves in the same way as a stack and exact opposite way as a FIFO queue. The cache evicts the block added most recently first without any regard to how often or how many times it was accessed before\n4. Least recently used (LRU): Discards the least recently used items first. This algorithm requires keeping track of what was used when, which is expensive if one wants to make sure the algorithm always discards the least recently used item. \n5. Most recently used (MRU): Discards, in contrast to LRU, the most recently used items first.\n6. Time aware least recently used (TLRU): The Time aware Least Recently Used (TLRU) is a variant of LRU designed for the situation where the stored contents in cache have a valid lifetime.\n7. Random replacement (RR): Randomly selects a candidate item and discards it to make space when necessary. This algorithm does not require keeping any information about the access history\n8. Least-frequently used (LFU): Those that are used least often are discarded first. This works very similar to LRU except that instead of storing the value of how recently a block was accessed, we store the value of how many times it was accessed. Thus an efficient cache replacement policy can significantly reduce the cache miss rate and hence lowering the possibility of a penalty of hundreds of cycles due to memory accesses.\n\nREFERENCES:\n1. https://www.geeksforgeeks.org/cache-m...\n2. https://en.wikipedia.org/wiki/Cache_(...)\n3. https://computer.howstuffworks.com/ca...",
          "valid": true
        },
        {
          "title": "Page Table",
          "videoId": "wMnffk4Gpsg",
          "transcript": "Non-Contiguous Memory Allocation:\nAs we all know , in non-contiguous memory allocation technique, different parts\nof the same process can be stored at different places in the main memory.\n\nThere are two popular techniques used for non-contiguous memory allocation-\n1. Paging\n2. Segmentation\nPaging:\n\n● Paging is a fixed size partitioning scheme.\n● In paging, secondary memory and main memory are divided into equal\nfixed size partitions.\n● The partitions of secondary memory are called as pages.\n● The partitions of main memory are called as frames.\n\nTranslating Logical Address into Physical Address-\nLogical address is address generated by CPU during execution whereas Physical\nAddress refers to location in memory unit(the one that is loaded into\nmemory).Address binding is the process of mapping from one address space to\nanother address space.\n● CPU always generates a logical address.\n● A physical address is needed to access the main memory.\nIn this situation, a unit named as Memory Management Unit comes into the\npicture. It converts the page number of the logical address to the frame number of\nthe physical address. The offset remains same in both the addresses. To perform\nthis task, Memory Management unit needs a special kind of mapping which is\ndone by page table.\nThe page table stores all the Frame numbers corresponding to the page numbers of\nthe page table. In other words, the page table maps the page number to its actual\nlocation (frame number) in the memory.\n\nValid-invalid bit attached to each entry in the page table.\n➢ “valid” indicates that the associated page is in the process’ logical address\nspace, and is thus a legal page.\n➢ “invalid” indicates that the page is not in the process’ logical address space.\nPage Table is a data structure used by the virtual memory system to store the\nmapping between logical addresses and physical addresses.\nFollowing steps are followed to translate logical address into physical address-\n\nStep-01:\n\nCPU generates a logical address consisting of two parts-\n1. Page Number\n2. Page Offset\n\n➢ Page Number specifies the specific page of the process from which CPU\nwants to read the data.\n➢ Page Offset specifies the specific word on the page that CPU wants to\nread.\n\nStep-02:\n\nFor the page number generated by the CPU,\n➢ Page Table provides the corresponding frame number (base address of the\nframe) where that page is stored in the main memory.\n\nStep-03:\n\n➢ The frame number combined with the page offset forms the required\nphysical address.\n➢ Frame number specifies the specific frame where the required page is\nstored.\n➢ Page Offset specifies the specific word that has to be read from that page.",
          "valid": true
        },
        {
          "title": "TLB",
          "videoId": "Nq-cQIgcOvA",
          "transcript": "Disadvantages in Paging:\nAs we all know Paging is a non-contiguous memory allocation technique in which the  Page Table maps the page number to the  corresponding frame number. But paging has some disadvantages like:\nSize of Page table can be very big and therefore it wastes main memory.\nCPU will take more time to read a single word from the main memory.\nHow to decrease the page table size?\nThe page table size can be decreased by increasing the page size but it will cause internal fragmentation and there will also be page wastage.\nWhat is internal fragmentation?\nIn fixed size partitioning if process size is smaller than the partition size some unused free space is left within the partition and these are called memory fragments/holes and their sum is called internal fragmentation .\nOther way is to use multilevel paging but that increases the effective access time therefore this is not a practical approach.\n \nWhat is Multilevel paging?\nMultilevel paging consists of two or more levels of page tables in a hierarchical manner. It is also known as hierarchical paging.\nHow to decrease the effective access time?\nCPU can use a register having the page table stored inside it so that the access time to access page table can become quite less but the registers are not cheaper and they are very small in comparison to the page table size therefore, this is also not a practical approach.\nTo overcome these drawbacks in paging, we have to look for a memory that is cheaper than the register and faster than the main memory so that the time taken by the CPU to access page table again and again can be reduced and it can focus only to access the actual word.\n \nTranslation Look aside Buffer:\nTo overcome this problem a high-speed cache is set up for page table entries called a Translation Look aside Buffer (TLB). Translation Look aside Buffer (TLB) is nothing but a special cache used to keep track of recently used transactions. It works on the principle of locality of reference. TLB contains page table entries that have been most recently used. \n \nStructure of TLB:\nTranslation Look aside Buffer (TLB) consists of two columns namely,\nPage Number\nFrame Number\n\nTranslating Logical Address into Physical Address Using TLB: \nIn a paging scheme using TLB, the logical address generated by the CPU is translated into the physical address using following steps. \nStep-01:\n \nCPU generates a logical address consisting of two parts:\nPage Number\nPage Offset\n \nStep-02:\n \nTLB is checked to see if it contains an entry for the referenced page number.\nThe referenced page number is compared with the TLB entries all at once.\n \nNow, two cases are possible-\n \nCase-01: If there is a TLB hit-\n \nIf TLB contains an entry for the referenced page number, a TLB hit occurs.\nIn this case, TLB entry is used to get the corresponding frame number for the referenced page number.\n \nCase-02: If there is a TLB miss-\n \nIf TLB does not contain an entry for the referenced page number, a TLB miss occurs.\nIn this case, page table is used to get the corresponding frame number for the referenced page number.\nThen, TLB is updated with the page number and frame number for future references.\n \n \n \n \nStep-03:\n \nAfter the frame number is obtained, it is combined with the page offset to generate the physical address.\nThen, physical address is used to read the required word from the main memory.\n\nEffective memory access time(EMAT) : \n\n \n \nBy the formula, we come to know that\nEffective access time will be decreased if the TLB hit rate is increased.\nEffective access time will be increased in the case of multilevel paging.\nThus TLB helps in faster translation of logical address into physical address.",
          "valid": true
        },
        {
          "title": "Pipeline",
          "videoId": "g-8giQRPBrk",
          "transcript": "In computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series, where the output of one element is the input of the next one. \nPipelining : \nPipelining is a process of arrangement of hardware elements of the CPU such that its overall performance is increased. Simultaneous execution of more than one instruction takes place in a pipelined processor. A pipelined processor does not wait until the previous instruction has been executed completely. Rather, it fetches the next instruction and begins its execution. It is also known as pipeline processing.\nLet us see a real life example that works on the concept of pipelined operation. Consider a water bottle packaging plant. Let there be 3 stages that a bottle should pass through, Inserting the bottle(I), Filling water in the bottle(F), and Sealing the bottle(S). Let us consider these stages as stage 1, stage 2 and stage 3 respectively. Let each stage take 1 minute to complete its operation.\nNow, in a non pipelined operation, a bottle is first inserted in the plant, after 1 minute it is moved to stage 2 where water is filled. Now, in stage 1 nothing is happening. Similarly, when the bottle moves to stage 3, both stage 1 and stage 2 are idle. But in pipelined operation, when the bottle is in stage 2, another bottle can be loaded at stage 1. Similarly, when the bottle is in stage 3, there can be one bottle each in stage 1 and stage 2. So, after each minute, we get a new bottle at the end of stage 3. Hence, the average time taken to manufacture 1 bottle is :\nWithout pipelining = 9/3 minutes = 3m\n\n\nI F S | | | | | |\n| | | I F S | | |\n| | | | | | I F S (9 minutes)\n\nWith pipelining = 5/3 minutes = 1.67m\nI F S | |\n| I F S |\n| | I F S (5 minutes)\n\nThus, pipelined operation increases the efficiency of a system.\nDesign of a basic pipeline:\nIn a pipelined processor, a pipeline has two ends, the input end and the output end. Between these ends, there are multiple stages/segments such that output of one stage is connected to input of next stage and each stage performs a specific operation.\nInterface registers are used to hold the intermediate output between two stages. These interface registers are also called latch or buffer.\nAll the stages in the pipeline along with the interface registers are controlled by a common clock.\nInstruction Pipelining:\nInstruction pipelining is a technique that implements a form of parallelism (Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously) called as instruction level parallelism within a single processor.\n \nPipeline Stages:\nRISC(Reduced instruction set computing) processor has 5 stage instruction pipeline to execute all the instructions in the RISC instruction set. Following are the 5 stages of RISC pipeline with their respective operations:\nStage 1 (Instruction Fetch)\nIn this stage the CPU reads instructions from the address in the memory whose value is present in the program counter.\nStage 2 (Instruction Decode)\nIn this stage, instruction is decoded and the register file is accessed to get the values from the registers used in the instruction.\nStage 3 (Instruction Execute)\nIn this stage, ALU operations are performed.\nStage 4 (Memory Access)\nIn this stage, memory operands are read and written from/to the memory that is present in the instruction.\nStage 5 (Write Back)\nIn this stage, computed/fetched value is written back to the register present in the instructions.\nAdvantages of Pipelining:\nThe cycle time of the processor is reduced.\nIt increases the throughput of the system\nIt makes the system reliable.\nDisadvantages of Pipelining:\nThe design of pipelined processor is complex and costly to manufacture.\nThe instruction latency ( The total number of clock cycles necessary to execute an instruction and produce the results of that instruction) is more.\n",
          "valid": true
        },
        {
          "title": "Hazards",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "IPC",
          "videoId": "oJp9drfCDp4",
          "transcript": "What is IPC?\n\nIn computer architecture, instructions per cycle (IPC), commonly called instructions per clock is one aspect of a processor's performance: the average number of instructions executed for each clock cycle.\n\nWhat is a clock cycle?\n\nA clock cycle, or simply a \"cycle,\" is a single electronic pulse of a CPU. During each cycle, a CPU can perform a basic operation such as fetching an instruction, accessing memory, or writing data.\n\nCalculation of IPC:\nThe calculation of IPC is done through running a set piece of code, calculating the number of machine-level instructions required to complete it, then using high-performance timers to calculate the number of clock cycles required to complete it on the actual hardware. The final result comes from dividing the number of instructions by the number of CPU clock cycles.\n \n \nExample for computing IPC:\nQUESTION:\n \nConsider A given program consists of a 100-instruction loop that is executed 2 times and if it takes 16,000 cycles to execute the program on a given system then calculate the IPC value for the program\nGIVEN:\n \nThe 100-instruction loop that is executed 42 times, so the total number of instructions executed is 4200 and 16,000 cycles are needed to execute the program.\nIPC =  number of instructions / number of CPU clock cycles\nnumber of instructions      = 4200\nnumber of CPU clock cycles = 16000\nIPC = 4200 / 16000 = 0.2625\n \nThe number of instructions per second and floating point operations per second for a processor can be derived by multiplying the number of instructions per cycle with the clock rate (cycles per second given in Hertz) of the processor. The number of instructions per second is an approximate indicator of the likely performance of the processor.\nIs the value of IPC constant?\nThe number of instructions executed per clock is not a constant for a given processor; it depends on how the particular software being run interacts with the processor, and indeed the entire machine, particularly the memory hierarchy. However, certain processor features tend to lead to designs that have higher-than-average IPC values; the presence of multiple arithmetic logic units (an ALU is a processor subsystem that can perform elementary arithmetic and logical operations) and short pipelines. \nRelation between instruction set and IPC:\nWhen comparing different instruction sets, a simpler instruction set may lead to a higher IPC figure than an implementation of a more complex instruction set using the same chip technology; however, the more complex instruction set may be able to achieve more useful work with fewer instructions. As such comparing IPC figures between different instruction sets (for example x86 vs. ARM) is usually meaningless\nClock speed vs. IPC:\nWhile clock speed tells you how many cycles a CPU can complete in a second, IPC tells you how many tasks a CPU can conduct in each cycle.\nTypical IPC values of some processors:\nThe die or processor die is a rectangular pattern on a wafer that contains circuitry to perform a specific function. Let us see some processors along with the IPC value.\nProcessor                                        IPC per die\nIntel 8088                            0.075\nIntel Pentium III                        3.4\nARM Cortex A7                        1.9\nRaspberry Pi 2                        4.744\nAMD Ryzen Threadripper 3990X            541.66\n \nFactors governing IPC:\nA given level of instructions per second can be achieved with a high IPC and a low clock speed , or from a low IPC and high clock speed . Both are valid processor designs, and the choice between the two is often dictated by history, engineering constraints, or marketing pressures. However, a high IPC with a high frequency will always give the best performance.",
          "valid": true
        },
        {
          "title": "Process Scheduling Algorithms",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Preemption",
          "videoId": "eaELronoUKY",
          "transcript": "WHAT IS PREEMPTION?\nIn computing, preemption is the act of temporarily interrupting a task being carried out by a computer system, without requiring its cooperation, and with the intention of resuming the task at a later time. Such changes of the executed task are known as context switches. It is normally carried out by a privileged task or part of the system known as a preemptive scheduler, which has the power to preempt, or interrupt, and later resume, other tasks in the system\nIn CPU Scheduling we have preemptive and non-preemptive scheduling. Let us discuss about them in detail.\n1. Preemptive Scheduling:\nPreemptive scheduling is used when a process switches from running state to ready state or from waiting state to ready state. The resources (mainly CPU cycles) are allocated to the process for limited amount of time and then it is taken away, and the process is again placed back in the ready queue if that process still has CPU burst time remaining (Burst time is the total time taken by a process for its execution on the CPU). That process stays in ready queue till it gets next chance to execute.\n \n \n2. Non-Preemptive Scheduling:\nNon-preemptive Scheduling is used when a process terminates, or a process switches from running to waiting state. In this scheduling, once the resources (like CPU cycles) are allocated to a process, the process holds the CPU till it gets terminated or it reaches a waiting state. In case of non-preemptive scheduling,  a process running in the CPU is not interrupted in the middle of the execution. \n \nAs we all know Gantt chart is a useful graphical tool which shows activities or tasks performed against time. \nLet us look into the Gantt chart of these two scheduling types with an example set of processes.",
          "valid": true
        },
        {
          "title": "Priority Inversion",
          "videoId": "fWt6Z-c8eIg",
          "transcript": "What is priority inversion?\nPriority inversion is an operating system scenario in which a higher priority process is preempted by a lower priority process. This implies the inversion of the priorities of the two processes.\n\nProblems due to Priority Inversion\nA system malfunction may occur if a high priority process is not provided the required resources.\nPriority inversion may also lead to implementation of corrective measures. These may include the resetting of the entire system.\nThe performance of the system can be reduced due to priority inversion. This may happen because it is imperative for higher priority tasks to execute promptly.\nSystem responsiveness decreases as high priority tasks may have strict time constraints or real time response guarantees.\nSometimes there is no harm caused by priority inversion as the late execution of the high priority process is not noticed by the system.\nSolutions to handle Priority Inversion\nSome of the solutions to handle priority inversion are given as follows \nPriority Ceiling\nAll of the resources are assigned a priority that is equal to the highest priority of any task that may attempt to claim them. This helps in avoiding priority inversion.\nDisabling Interrupts\nThere are only two priorities in this case i.e. interrupts disabled and pre-emptible. So priority inversion is impossible as there is no third option.\nPriority Inheritance\nThis solution temporarily elevates the priority of the low priority task that is executing to the highest priority task that needs the resource. This means that medium priority tasks cannot intervene and lead to priority inversion.\nNo blocking\nPriority inversion can be avoided by avoiding blocking as the low priority task blocks the high priority task.\nRandom boosting\nThe priority of the ready tasks can be randomly boosted until they exit the critical section.\n\nIn some cases, priority inversion can occur without causing immediate harm but there are also many situations in which priority inversion can cause serious problems.",
          "valid": true
        },
        {
          "title": "Interrupts",
          "videoId": "x0Da4OEmVUw",
          "transcript": "Before getting into what are interrupts , let us first get to know what is polling?\nWhat is Polling?\nThe state of continuous monitoring is known as polling. The microcontroller keeps checking the status of other devices; and while doing so, it does no other operation and consumes all its processing time for monitoring. This problem can be addressed by using interrupts.\nIn the interrupt method, the controller responds only when an interruption occurs. Thus, the controller is not required to regularly monitor the status (flags, signals etc.) of interfaced and inbuilt devices.\nWhat are interrupts?\nAn interrupt is a signal to the processor emitted by hardware or software indicating an event that needs immediate attention. Whenever an interrupt occurs, the controller completes the execution of the current instruction and starts the execution of an Interrupt Service Routine (ISR) or Interrupt Handler. ISR tells the processor or controller what to do when the interrupt occurs. \n\n\n\nProgram Execution With And Without Interrupts:\n\nInterrupts v/s Polling:\nHere is an analogy that differentiates an interrupt from polling −\nInterrupt\nPolling\nAn interrupt is like a shopkeeper. If one needs a service or product, he goes to him and apprises him of his needs. In case of interrupts, when the flags or signals are received, they notify the controller that they need to be serviced.\nThe polling method is like a salesperson. The salesman goes from door to door while requesting to buy a product or service. Similarly, the controller keeps monitoring the flags or signals one by one for all devices and provides service to whichever component that needs its service.\n\nThe interrupts can be either hardware interrupts or software interrupts.\nHardware Interrupt:\nA hardware interrupt is an electronic alerting signal sent to the processor from an external device, like a disk controller or an external peripheral. For example, when we press a key on the keyboard or move the mouse, they trigger hardware interrupts which cause the processor to read the keystroke or mouse position.\nSoftware Interrupt:\nA software interrupt is caused either by an exceptional condition or a special instruction in the instruction set which causes an interrupt when it is executed by the processor. Software interrupt instructions work similar to subroutine calls. For example, if the processor's arithmetic logic unit runs a command to divide a number by zero, to cause a divide-by-zero exception, thus causing the computer to abandon the calculation or display an error message. \nInterrupt Service Routine:\nFor every interrupt, there must be an interrupt service routine (ISR), or interrupt handler. When an interrupt occurs, the microcontroller runs the interrupt service routine. For every interrupt, there is a fixed location in memory that holds the address of its interrupt service routine, ISR. The table of memory locations set aside to hold the addresses of ISRs is called as the Interrupt Vector Table.\n\n\n\nSteps to Execute an Interrupt:\nWhen an interrupt gets active, the microcontroller goes through the following steps \nThe microcontroller closes the currently executing instruction and saves the address of the next instruction  from program counter (PC) on the stack.\nIt also saves the current status of all the interrupts internally (i.e., not on the stack).\nIt jumps to the memory location of the interrupt vector table that holds the address of the interrupt service routine.\nThe microcontroller gets the address of the ISR from the interrupt vector table and jumps to it. It starts to execute the interrupt service subroutine, which is RETI (return from interrupt).\nUpon executing the RETI instruction, the microcontroller returns to the location where it was interrupted. First, it gets the program counter (PC) address from the stack by popping the top bytes of the stack into the PC. Then, it start to execute from that address.",
          "valid": true
        },
        {
          "title": "Profiling",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "File System",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Links",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Belady's Anomaly",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Setuid Bit",
          "videoId": "",
          "transcript": "",
          "valid": false
        }
      ]
    },
    {
      "name": "Parallelization",
      "content": [
        {
          "title": "Barriers",
          "videoId": "cBUQRJ4PspU",
          "transcript": "The fundamental idea behind parallelisation is to speed processing up by performing several tasks in parallel. However, in most non-trivial programs, different threads will need to use results of subproblems computed by other processors. They can do this by storing these intermediate results in a shared or a global memory space.\n\nHowever, an obvious problem arises here. Suppose T1 and T2 are two threads working in parallel. For the sake of argument, let us assume that T1 takes a longer time to execute than T2 does. Now, when T2 reaches a point in its execution where it needs to use a value computed by T1, it reads a designated location in the shared memory. However, if T1 hasn’t computed and written the required value into that location yet, T2 ends up with a junk value which would not be what the programmer had intended.\n\nClearly, we need some way to guarantee that T2 only reads from memory location after T1 has written into it. In other words, we need T1 and T2 to coordinate with each other. We need some sort of synchronisation technique to fix this. In fact, the need for synchronisation is one of the most fundamental problems in parallel computing. One of the simplest synchronisation techniques involves using barriers. A barrier is a point in the parallel program where all threads meet before any thread moves ahead. Each thread that reaches a barrier pauses its execution and waits for all threads to reach the barrier before continuing with its execution.\n\nFor example, if the instructions that T1 needs to execute are P1, P2, Barrier, P3. And the ones which T2 needs to execute are Q1, Barrier, Q2, Q3, it is possible for statements P1 and/or P2 to run in parallel with Q1 but not in parallel with Q2. Similarly, P3 can never run in parallel with Q1 but can run in parallel with Q2 and/or Q3. Assuming Q1 finishes executing before P2 does, T2 pauses its execution at the barrier until T1 finishes executing P2 and reaches the barrier too. Only then, do the threads begin to execute P3 and Q2. Every barrier can be divided into two phases. The arrival step also known as the trapping step, puts the threads arriving at the barrier into a wait state. A thread cannot continue it's execution while in the arrival step. Once all the threads have been trapped by the barrier, we enter the departure step and all the threads are released from the wait state and can resume execution.\n\nThere are several algorithms that can be used for implementing these two steps. The algorithms can be classified into three categories:\n1) The linear barrier\n2) The Tree barrier\n3) The Butterfly barrier\n\nHere are some situations where the concept of barriers would arise naturally in the real world:\n1)In a classroom, the teacher has to wait until all the students have noted down what’s on the board until she can clear the board and start a new topic.\n2) During the construction of a multi-storeyed house, the construction of each pillar can be considered as a thread. We have to wait until all the pillars of the first floor are laid before we start laying out pillars of the second floor.\n\nBarriers can also be used in some unconventional ways.\n1) Timing a portion of a multi-threaded process: We know that no thread can proceed beyond a barrier until all the threads have reached it. So, we can use barriers as a start point since all the threads start running(from the barrier) in the timed code at the same time.\n2) Debugging: Debugging parallel computation becomes very difficult unless we have an idea of how far each of our threads have reached in the program. Suppose there are 2 threads T1 and T2 and that at some point of time,T1 must access a value computed by T2. If there was a bug in the code executed by T2, T1 may end up reading an incorrect value. It can be difficult to figure out the location of the bug in the parallel program. Introducing a barrier just before this value is computed by T2 can help to identify and isolate this bug.",
          "valid": true
        },
        {
          "title": "Data races",
          "videoId": "16X9nHNQfI8",
          "transcript": "We will try to understand a data race using the three Ws: What, When, Why.\n\nA datarace occurs in a parallel program when multiple threads access the same memory location and at least one of those accesses is a write. It is a property of the execution of a program.\n\nA thread analyser is responsible for detecting data races during the execution of a multi-threaded program. Multiple threads of the same process share a single address space and access the same memory location. At least one of the threads’ accesses is a write. These accesses are not ordered by any forms of synchronisation operations, such as locks and condition variables. The order of accesses is non-deterministic when these three conditions hold. Therefore our computation results vary depending on the order of these accesses and particularly where the write accesses are placed in that order. When there is no write, the value in the address location will remain the same and regardless of when a thread accesses, the value accessed will be the same. Let us observe the execution of the two threads T1 and T2. T1 and T2 share a variable x and at least one of the accesses is a write. We can see here that the order of accesses is important in determining the end result. Here are two different orders for the execution of these threads.\n\nIn the first execution the value of y printed is 10. Whereas in the second execution the value of y printed is 15. We have to keep in mind that the order of execution cannot be determined because the compiler and hardware strive for better performance and they can restructure the code entirely.\n\nData Races in real life:\nMoney transfer in Bank Accounts. We have to wait until we have sufficient money in our bank accounts for a transfer to be valid. Hence we have to receive the money first and then only can we send it. Here the relative order of the transfers is important.\n\nHow to deal with data races:\nData races are synchronisation bugs. Therefore data races are handled by synchronising accesses to shared memory. Various ways to synchronise are locks, atomics and barriers.",
          "valid": true
        },
        {
          "title": "Critical section",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Mutual exclusion",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Processes and Threads",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Atomicity",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Sequential consistency",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Linearizability",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Locks",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Lock-free vs. Wait-free data structure",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Forms of parallelism",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Monitor",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Synchronous vs. Asynchronous execution",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Conditional variables",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Semaphores",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Coarse-grained vs. Fine-grained synchronization",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Optimistic vs. Lazy synchronization",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "ABA problem",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Sorting network",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Transactions",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Sleep and wakeup",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Transactional memory",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Memory consistency",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Forks and Joins",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Multicore, Many-core and Distributed processing",
          "videoId": "",
          "transcript": "",
          "valid": false
        }
      ]
    },
    {
      "name": "OpenMP Programming",
      "content": [
        {
          "title": "Parallel Region",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Parallel For",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Sections",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Reduction",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Barrier",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Master Thread",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Single",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Private Variables",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Critical Section",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Atomics",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Flush",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Asynchronous Execution (nowait)",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Ordered",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "False Sharing",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Strided Access",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Schedule",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Loop Carried Dependence",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Conditional (if)",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Data Copying (copyin, copyprivate)",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Functions",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Environment Variables",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Amdahl's Law",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Speedup",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Thread Affinity",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Directive Based Programming",
          "videoId": "",
          "transcript": "",
          "valid": false
        }
      ]
    },
    {
      "name": "MPI Programming",
      "content": [
        {
          "title": "Distributed System",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Point-to-Point Communication",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Broadcast",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Topology",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Send / Receive Messages",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Gather",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Reduce",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Barrier",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "BSP",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Cohesion",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Coupling",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Overlapped Communication and Computation",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Blocking and Non-blocking Communication",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Data Transfer: Serialization and Deserialization",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Communication Bandwidth",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Network Latency",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Leader Election",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Get, Put, Accumulate",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Rank",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "mpiexec / mpirun",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Persistent Communication",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Scan",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Flush and Sync",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Process Creation",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "One Sided Communication",
          "videoId": "",
          "transcript": "",
          "valid": false
        }
      ]
    },
    {
      "name": "GPU Programming",
      "content": [
        {
          "title": "Thread Block",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Kernel",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Warp",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Flynn's Taxonomy",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Memory Coalescing",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Thread Divergence",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Shared Memory",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Texture Memory",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Warp Voting",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Reduction",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Prefix Sum",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "PTX",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "CUDA",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "OpenCL",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Stream",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Event",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Pinned Memory",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Unified Virtual Memory",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Peer Access",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Thread Block Barrier",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Thrust Library",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Dynamic Parallelism",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Heterogeneous Programming",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "Host-Device Functions",
          "videoId": "",
          "transcript": "",
          "valid": false
        },
        {
          "title": "OpenACC",
          "videoId": "",
          "transcript": "",
          "valid": false
        }
      ]
    }
  ]
}